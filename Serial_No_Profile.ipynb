{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part Data shape: (5, 9)\n",
      "\n",
      "Total number of violations in the DataFrame: 5\n",
      "\n",
      "Discrepancies between the original and updated DataFrames:\n",
      "+----+----------+------------+----------------------------------------+---------+----------------------------------+---------------------------+----------------------+--------------------+-------------------+-------------+\n",
      "|    |   Row ID |   Material | Description                            | Plant   | Plant-Specific Material Status   | Batch Management(Plant)   | Serial No. Profile   | Replacement Part   | Used in a S-bom   |   Violation |\n",
      "|----+----------+------------+----------------------------------------+---------+----------------------------------+---------------------------+----------------------+--------------------+-------------------+-------------|\n",
      "|  0 |        2 |      50300 | HOG-10 ENCODER                         | 43S1    | ZS                               |                           | ZPP2                 |                    |                   |           1 |\n",
      "|  1 |        3 |      50300 | HOG-10 ENCODER                         | 51S1    | ZS                               |                           | ZPP2                 |                    |                   |           1 |\n",
      "|  2 |        4 |     106426 | ULTRA SONIC ANEMOMETER 2D              | 43S1    | ZS                               |                           | ZPP2                 |                    |                   |           1 |\n",
      "|  3 |        5 |     106426 | ULTRA SONIC ANEMOMETER 2D              | 51S1    | ZS                               |                           | ZPP2                 |                    |                   |           1 |\n",
      "|  4 |        6 |     733655 | ABB GENERATOR V42 M2CG 400XL 4B3 600KW | 43S1    | Z3                               |                           | ZPP2                 |                    |                   |           1 |\n",
      "+----+----------+------------+----------------------------------------+---------+----------------------------------+---------------------------+----------------------+--------------------+-------------------+-------------+\n",
      "\n",
      "Processed data has been exported to MongoDB's 'Processed v1' collection.\n",
      "\n",
      "Number of documents in the processed data collection: 5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymongo\n",
    "from tabulate import tabulate\n",
    "\n",
    "# MongoDB Connection\n",
    "def connect_to_mongo():\n",
    "    # Replace with your MongoDB connection details\n",
    "    client = pymongo.MongoClient(\n",
    "        'mongodb+srv://SkyrichSolutions:admin@skyrich-solutions.4hwg7.mongodb.net/?retryWrites=true&w=majority&appName=SkyRich-Solutions'\n",
    "    )\n",
    "    db = client['Skyrich-Unprocessed']  # Replace with your database name\n",
    "    unprocessed_collection = db['UnProcessed v1']  # Collection for unprocessed data\n",
    "    processed_collection = db['Processed v1']  # Collection for processed data\n",
    "    return unprocessed_collection, processed_collection\n",
    "\n",
    "# Function to load data from MongoDB's UnprocessedData collection\n",
    "def load_data_from_mongo():\n",
    "    unprocessed_collection, _ = connect_to_mongo()\n",
    "    cursor = unprocessed_collection.find()  # Retrieve all documents\n",
    "    data_list = list(cursor)\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df.drop(columns=['_id'], inplace=True, errors='ignore')  # Drop the _id field\n",
    "    return df\n",
    "\n",
    "# Load the unprocessed data into a DataFrame\n",
    "Part_Analysis = load_data_from_mongo()\n",
    "\n",
    "# Display the shape of the DataFrame\n",
    "print(\"Part Data shape:\", Part_Analysis.shape)\n",
    "\n",
    "# Define the serial number profiles that should be labeled as \"Replacement part B\"\n",
    "serial_no_profiles = ['ZPP2', 'ZPP8', 'ZCS1']\n",
    "\n",
    "# Add a 'Violation' column to flag rows that are discrepancies\n",
    "Part_Analysis['Violation'] = (\n",
    "    (Part_Analysis['Serial No. Profile'].isin(serial_no_profiles)) & \n",
    "    (Part_Analysis['Replacement Part'] != 'B')\n",
    ").astype(int)  # 1 = Violation, 0 = No Violation\n",
    "\n",
    "# Calculate the total number of violations in the DataFrame\n",
    "total_violations = Part_Analysis['Violation'].sum()\n",
    "print(f\"\\nTotal number of violations in the DataFrame: {total_violations}\")\n",
    "\n",
    "# Extract rows with violations\n",
    "discrepancies = Part_Analysis[Part_Analysis['Violation'] == 1]\n",
    "\n",
    "if not discrepancies.empty:\n",
    "    # Add a Row ID column (+2 offset)\n",
    "    discrepancies['Row ID'] = discrepancies.index + 2\n",
    "\n",
    "    # Rearrange columns to place 'Row ID' as the first column\n",
    "    columns = ['Row ID'] + [col for col in discrepancies.columns if col != 'Row ID']\n",
    "    discrepancies = discrepancies[columns]\n",
    "\n",
    "    # Display the discrepancies\n",
    "    print(\"\\nDiscrepancies between the original and updated DataFrames:\")\n",
    "    print(tabulate(discrepancies, headers='keys', tablefmt='psql'))\n",
    "else:\n",
    "    print(\"\\nNo discrepancies detected in the data set.\")\n",
    "\n",
    "# Process the data\n",
    "processed_Part_Analysis = Part_Analysis.copy()\n",
    "\n",
    "# Update the 'Replacement Part' column to 'B' for the discrepancies\n",
    "processed_Part_Analysis.loc[\n",
    "    processed_Part_Analysis['Serial No. Profile'].isin(serial_no_profiles),\n",
    "    'Replacement Part'\n",
    "] = 'B'\n",
    "\n",
    "# Insert the processed data into MongoDB (Processed v1 collection)\n",
    "_, processed_collection = connect_to_mongo()  # Get only the processed collection\n",
    "processed_collection.delete_many({})  # Clear previous data in the processed collection\n",
    "processed_collection.insert_many(processed_Part_Analysis.to_dict('records'))\n",
    "\n",
    "print(\"\\nProcessed data has been exported to MongoDB's 'Processed v1' collection.\")\n",
    "\n",
    "# Optional: Verify processed data count in MongoDB\n",
    "processed_data_count = processed_collection.count_documents({})\n",
    "print(f\"\\nNumber of documents in the processed data collection: {processed_data_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
